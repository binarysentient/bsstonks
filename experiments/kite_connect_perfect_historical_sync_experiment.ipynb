{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install --upgrade kiteconnect\r\n",
    "!pip install python-dotenv\r\n",
    "!pip install python-dateutil"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import logging\r\n",
    "import os\r\n",
    "from kiteconnect import KiteConnect\r\n",
    "from dotenv import load_dotenv\r\n",
    "load_dotenv()\r\n",
    "import bs_threading\r\n",
    "import importlib\r\n",
    "import time\r\n",
    "from datetime import datetime, timedelta\r\n",
    "import dateutil\r\n",
    "import pandas as pd\r\n",
    "importlib.reload(bs_threading)\r\n",
    "from bs_threading import bs_threadify, bs_make_throttle_ready_func\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "KITE_CONNECT_API_KEY = os.getenv(\"KITE_CONNECT_API_KEY\")\r\n",
    "KITE_CONNECT_API_SECRET = os.getenv(\"KITE_CONNECT_API_SECRET\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "logging.basicConfig(level=logging.DEBUG)\r\n",
    "\r\n",
    "kite = KiteConnect(api_key=KITE_CONNECT_API_KEY)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get the url based on your api key, which will have request token, this is in order to verify the app itself(bsstonks) is valid and it'll get redirected to the callback url"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(kite.login_url())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Given we have request token and redirected to our callback with it, our app is valid, but is the app in the hands/server of the right person or not? only API secret can verify that. \n",
    "### We'll pass that api secret with the request token to create a session which give us Access_token, to the rightful person that we are"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Redirect the user to the login url obtained\n",
    "# from kite.login_url(), and receive the request_token\n",
    "# from the registered redirect url after the login flow.\n",
    "# Once you have the request_token, obtain the access_token\n",
    "# as follows.\n",
    "\n",
    "kite_session_data = kite.generate_session(\"azmU6LOmL6sqQmNGjXtVBdmoGpKr7623\", api_secret=KITE_CONNECT_API_SECRET)\n",
    "print(\"THE TOKEN:\",kite_session_data[\"access_token\"])\n",
    "kite.set_access_token(kite_session_data[\"access_token\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kite.set_access_token(\"G141alqP4huPnGMMXFwb2OIGjvu3q9I5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kite.orders()\n",
    "kite.holdings()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_instruments = kite.instruments()\n",
    "all_instruments_nse = kite.instruments(exchange=\"NSE\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_all_instruments = pd.DataFrame(all_instruments)\n",
    "os.makedirs(\"input/kite_instruments\", exist_ok=True)\n",
    "df_all_instruments.to_csv(\"input/kite_instruments/instrument_list.csv\", index=False)\n",
    "df_all_instruments"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_all_instruments[\"segment\"].unique()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "print(\"Total Instruments:\", len(all_instruments))\n",
    "print(\"Total EQ\",len([x for x in all_instruments if x['instrument_type']==\"EQ\"]))\n",
    "print(\"Sample instsrument:\", all_instruments[0])\n",
    "all_tcs_match = [x for x in all_instruments if \"tcs\" in x['tradingsymbol'].lower()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Total TCS match:\", len(all_tcs_match))\n",
    "for x in all_tcs_match:\n",
    "    print(x['tradingsymbol'], x['name'], \"InstrumentType:\", x['instrument_type'],'Exchange:',x['exchange'], \"InstrumentToken:\", x['instrument_token'])\n",
    "all_tcs_match"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kite.quote([\"2953217\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#TCS NSE TOKEN: 2953217\n",
    "# day : 2000 days  = 2000 entries\n",
    "# 60minute : 400 days = 400 * 24 = 9600 entries\n",
    "# 30minute : 200 days = \n",
    "# 15minute : 200 days =\n",
    "# 5minute : 100 days = \n",
    "# 3minute : 100 days =\n",
    "# minute : 60 days = \n",
    "interval_data_span_limit = {\n",
    "    'day': 2000,\n",
    "    '60minute':400,\n",
    "    '30minute':200,\n",
    "    '15minute':200,\n",
    "    '5minute':100,\n",
    "    '3minute':100,\n",
    "    'minute':60,\n",
    "}\n",
    "kite.historical_data(2953217, \"2021-06-15\", \"2021-06-16\", \"day\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kite.historical_data(2953217, \"2021-06-15\", \"2021-06-16\", \"hour\", oi=True)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We want to keep 3 request per second limit of zerodha\n",
    "- ### we'll use multiple threads\n",
    "- ### we'll use the shared throttle function which only returns true if we haven't made request in last x(can be sub second) seconds"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "throttle_ready_func = bs_make_throttle_ready_func(min_interval_second=1.0/3.0)\n",
    "def worker_func(data):\n",
    "    while not throttle_ready_func():\n",
    "        time.sleep(0.001)\n",
    "    print(\"this is the real life\")\n",
    "\n",
    "worker_data = [0] * 6\n",
    "last_time = time.time()\n",
    "queue = bs_threadify(worker_data, worker_func, num_threads=16)\n",
    "print(\"Time took\", time.time()-last_time)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Okay then throttle seems to be working with multithreading, let's create the data fetcher"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_instruments_nse_eq = [x for x in all_instruments_nse if x['instrument_type']==\"EQ\"]\n",
    "print(len(all_instruments_nse),len(all_instruments_nse_eq))\n",
    "all_instruments_nse_eq[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def kite_instrument_to_filename(instrument_dict, interval=\"day\", expiry=\"1mo\"):\n",
    "    kite_instrument_token = instrument_dict['instrument_token']\n",
    "    trading_symbol = instrument_dict['tradingsymbol']\n",
    "    instrument_type = instrument_dict['instrument_type']\n",
    "    segment = instrument_dict['segment']\n",
    "    name = instrument_dict['name']\n",
    "    exchange = instrument_dict['exchange']\n",
    "    if exchange == \"NFO\" and (instrument_type == \"CE\" or instrument_type == \"PE\"):\n",
    "        trading_symbol = trading_symbol.replace(name,\"\")\n",
    "        trading_symbol = trading_symbol[5:]\n",
    "        trading_symbol = name + trading_symbol\n",
    "        trading_symbol += expiry\n",
    "    # we've tested that _ is valid seperator by evaluating for all the instruments (86k)\n",
    "    result_file = f\"{kite_instrument_token}_{trading_symbol}_{instrument_type}_{segment}_{exchange}_{interval}.csv\"\n",
    "    return result_file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kite_instrument_to_filename([x for x in all_instruments if x['segment'] == \"NFO-OPT\" and x['name'].startswith(\"REL\")][0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def make_sync_single_symbol_worker_func(throttle_ready_func, output_path = None, expiry = None):\n",
    "    #NOTE: input to our stock prediction/training system\n",
    "    if output_path is None:\n",
    "        output_path = \"input/kite_historical\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    def sync_single_symbol_worker_func(data):\n",
    "        nonlocal throttle_ready_func\n",
    "        nonlocal expiry\n",
    "        nonlocal output_path\n",
    "        # data['instrument_dict'] this should be instrument dictionary that zerodha returns\n",
    "        # data['interval'] \n",
    "        \n",
    "        data_interval = data['interval']\n",
    "        if 'fetch_past' not in data:\n",
    "            data['fetch_past'] = True\n",
    "            \n",
    "        interval_span_days = interval_data_span_limit[data_interval]\n",
    "        # we've tested that _ is valid seperator by evaluating for all the instruments (86k)\n",
    "        result_file = kite_instrument_to_filename(data['instrument_dict'], interval=data['interval'], expiry=expiry)\n",
    "        result_file_path = os.path.join(output_path,result_file)\n",
    "        df_to_sync = None\n",
    "        if not os.path.exists(result_file_path):\n",
    "            df_to_sync = pd.DataFrame([],columns=[\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
    "#             df_to_sync.to_csv(result_file_path, index=False, header=True)\n",
    "        else:\n",
    "            df_to_sync = pd.read_csv(result_file_path,index_col=False)\n",
    "\n",
    "\n",
    "\n",
    "        #     \n",
    "        # Past direction\n",
    "        while True and (data['fetch_past'] or df_to_sync.shape[0]==0):\n",
    "\n",
    "            #IMPORTANT: THROTTLE LIMIT MUST BE RESPECTED\n",
    "            while not throttle_ready_func():\n",
    "                time.sleep(0.001)\n",
    "\n",
    "            end = datetime.now()\n",
    "            start = end - timedelta(days=interval_span_days-1)\n",
    "\n",
    "            if df_to_sync.shape[0] > 0:\n",
    "                end = df_to_sync.iloc[0][\"date\"]\n",
    "                if type(end) == str:\n",
    "                    end = dateutil.parser.parse(end)\n",
    "                end = end - timedelta(days=1)\n",
    "                #dates are inclusive so start date (historical date) needs to be adjusted\n",
    "                start = end - timedelta(days=interval_span_days-1)\n",
    "\n",
    "            historical_data = kite.historical_data(kite_instrument_token, start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\"), data_interval, oi=True)\n",
    "            for x in historical_data:\n",
    "                x['date'] = str(x['date'])\n",
    "\n",
    "            # WHEN API RETURNS NO DATA; EXIT.\n",
    "            if len(historical_data) == 0:\n",
    "                break\n",
    "\n",
    "\n",
    "            df_to_sync_more_hist = pd.DataFrame(historical_data)\n",
    "\n",
    "            df_to_sync = df_to_sync_more_hist.append(df_to_sync, ignore_index=True)      \n",
    "            df_to_sync.to_csv(result_file_path, index=False)\n",
    "\n",
    "            # if it returns less than possible working days of data then just \n",
    "            if len(historical_data) < 3 or dateutil.parser.parse(historical_data[-1]['date']) - dateutil.parser.parse(historical_data[0]['date']) < timedelta(days=int(interval_span_days*0.5 - 15)):\n",
    "                break\n",
    "\n",
    "\n",
    "        #     \n",
    "        # Future direction\n",
    "        while True:\n",
    "\n",
    "            #IMPORTANT: THROTTLE LIMIT MUST BE RESPECTED\n",
    "            while not throttle_ready_func():\n",
    "                time.sleep(0.001)\n",
    "\n",
    "            start = datetime.now()\n",
    "            end = start + timedelta(days=interval_span_days-1)\n",
    "\n",
    "            if df_to_sync.shape[0] > 0:\n",
    "                start = df_to_sync.iloc[-1][\"date\"]\n",
    "                if type(start) == str:\n",
    "                    start = dateutil.parser.parse(start)\n",
    "                # what if we run it while the stock market is open, and run it again after it closes\n",
    "                # today's data must be overwritten/appended\n",
    "                start = start - timedelta(days=1)\n",
    "                #dates are inclusive so start date (historical date) needs to be adjusted\n",
    "                end = start + timedelta(days=interval_span_days-1)\n",
    "\n",
    "\n",
    "            historical_data = kite.historical_data(kite_instrument_token, start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\"), data_interval, oi=True)\n",
    "            \n",
    "            for x in historical_data:\n",
    "                x['date'] = str(x['date'])\n",
    "\n",
    "            if len(historical_data) == 0:\n",
    "                break\n",
    "            # WHEN API RETURNS NO DATA; EXIT.\n",
    "\n",
    "\n",
    "            df_to_sync_more_hist = pd.DataFrame(historical_data)\n",
    "            # rewrite last few days of new data onto old data; this takes care of running it during trdaing window\n",
    "            overlaps_count = df_to_sync[df_to_sync['date']>=str(historical_data[0]['date'])].shape[0]\n",
    "            \n",
    "\n",
    "            df_to_sync = df_to_sync.iloc[:-overlaps_count]\n",
    "            \n",
    "            df_to_sync = df_to_sync.append(df_to_sync_more_hist, ignore_index=True)      \n",
    "            df_to_sync.to_csv(result_file_path, index=False)\n",
    "\n",
    "            # if it returns less than possible working days of data then just \n",
    "            if len(historical_data) < 3 or dateutil.parser.parse(historical_data[-1]['date']) - dateutil.parser.parse(historical_data[0]['date']) < timedelta(days=int(interval_span_days*0.5 - 15)):\n",
    "                break\n",
    "\n",
    "    return sync_single_symbol_worker_func\n",
    "# USage:\n",
    "\n",
    "importlib.reload(bs_threading)\n",
    "sync_single_symbol_worker_func = make_sync_single_symbol_worker_func(bs_make_throttle_ready_func(min_interval_second=1.01/3.0))\n",
    "sync_single_symbol_worker_func({'instrument_dict':all_tcs_match[0],'interval':'day'})"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Yay! we made it, we made the perfect sync code!!!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's do day wise sync"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sync_instruments(instuments_list, interval=\"15minute\", need_daily_exist=False, fetch_past=True):\n",
    "    should_terminate=False\n",
    "\n",
    "    sync_single_symbol_worker_func = make_sync_single_symbol_worker_func(bs_make_throttle_ready_func(min_interval_second=1.01/3.0))\n",
    "    output_path = \"input/kite_historical\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    symbol_worker_data = []\n",
    "    for instrument_dict in instuments_list:\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        # we've tested that _ is valid seperator by evaluating for all the instruments (86k)\n",
    "        result_file = f\"{kite_instrument_token}_{trading_symbol}_{instrument_type}_{segment}_{exchange}_day.csv\"\n",
    "        result_file_path = os.path.join(output_path,result_file)\n",
    "        if os.path.exists(result_file_path) or not need_daily_exist:\n",
    "            symbol_worker_data.append({'instrument_dict':instrument_dict,'interval':interval,'fetch_past':fetch_past})\n",
    "#             sync_single_symbol_worker_func({'instrument_dict':instrument_dict,'interval':interval})\n",
    "\n",
    "#     print(len(symbol_worker_data), \" out of \", len(all_instruments_nse))\n",
    "    def should_terminate_func():\n",
    "        nonlocal should_terminate\n",
    "        return should_terminate\n",
    "    \n",
    "    bs_threadify(symbol_worker_data, sync_single_symbol_worker_func, num_threads=8, should_terminate_func=should_terminate_func)\n",
    "    should_terminate = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sync_single_symbol_worker_func = make_sync_single_symbol_worker_func(bs_make_throttle_ready_func(min_interval_second=1.01/3.0))\n",
    "\n",
    "# symbol_worker_data = []\n",
    "# for instrument_dict in all_instruments_nse:\n",
    "#     symbol_worker_data.append({'instrument_dict':instrument_dict,'interval':'day'})\n",
    "# bs_threadify(symbol_worker_data, sync_single_symbol_worker_func, num_threads=8)\n",
    "sync_instruments(all_instruments_nse, interval='day',fetch_past=False)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### There are some useless symbols there with no data, only do sub day interval for those for which the day data exist"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sync_instruments(all_instruments_nse, interval='15minute', need_daily_exist=True, fetch_past=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sync_instruments(all_instruments_nse, interval='5minute', need_daily_exist=True, fetch_past=True)\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# historical_data = kite.historical_data(264713, \"2021-06-17\", \"2022-01-02\", \"15minute\", oi=True)\n",
    "# pd.DataFrame(historical_data)\n",
    "nifty_50_instruments = [x for x in all_instruments_nse if \"nifty 50\" in x['tradingsymbol'].lower()]\n",
    "\n",
    "nifty_50_instruments\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# all_instruments_nse_eq[all_instruments_nse_eq[\"tradingsymbol\"].str.lower().str.startswith(\"nifty\") & all_instruments_nse_eq[\"name\"].str.lower().str.startswith(\"nifty\")]\n",
    "print(\"Total instruments to fetch:\", len(nifty_50_instruments))\n",
    "sync_instruments(nifty_50_instruments, interval=\"day\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "historical_data = kite.historical_data(136330244, \"2015-04-01\", \"2015-05-10\", \"day\", oi=True)\n",
    "        \n",
    "df_to_sync_more_hist = pd.DataFrame(historical_data)\n",
    "df_to_sync_more_hist2 = pd.DataFrame(historical_data)\n",
    "df_to_sync_more_hist2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}